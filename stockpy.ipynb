{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impoting the libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "import sklearn\n",
    "from matplotlib.finance import candlestick_ohlc\n",
    "import matplotlib.dates as mdates\n",
    "import bs4 as bs\n",
    "import urllib.request as urlb\n",
    "import pickle\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "#import mpld3\n",
    "%matplotlib notebook\n",
    "\n",
    "#mpld3.enable_notebook()\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asigning the start date and end date \n",
    "start = dt.datetime(1985, 1, 1) # collect from this date\n",
    "end = dt.datetime.today() #to this date.\n",
    "stock  = 'AAPL' #using Apple's stock data\n",
    "df = web.DataReader(stock,'yahoo',start,end) #fetches the data from Yahoo and stores it in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'Adj Close':'adj_close'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AAPL.csv', parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data[['adj_close','High','Low','Open','Close']].plot()\n",
    "plt.title(\"Stock Prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling average method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column to add rolling average\n",
    "\n",
    "#data['100ma'] = data['adj_close'].rolling(window=100).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THe first 100 rows cant have the calculation.\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first 100 rows will be deleted due to dropna\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "# #visualiziation\n",
    "# ax1 = plt.subplot2grid((6,1), (0,0),rowspan=5,colspan=1)\n",
    "# ax2 = plt.subplot2grid((6,1), (5,0),rowspan=1,colspan=1, sharex=ax1)\n",
    "\n",
    "# ax1.plot(data.index,data['adj_close'])\n",
    "# ax1.plot(data.index,data['100ma'])\n",
    "# ax2.bar(data.index,data['Volume'])\n",
    "\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_ohlc=data['adj_close'].resample('10D').ohlc()#creates open high low close\n",
    "data_volume= data['Volume'].resample('10D').sum()\n",
    "data_ohlc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohlc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#candlestick_ohlc requires mdates and ohlc.\n",
    "data_ohlc['Date'] = data_ohlc['Date'].map(mdates.date2num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohlc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "#visualiziation\n",
    "ax1 = plt.subplot2grid((6,1), (0,0),rowspan=5,colspan=1)\n",
    "ax2 = plt.subplot2grid((6,1), (5,0),rowspan=1,colspan=1, sharex=ax1)\n",
    "\n",
    "ax1.xaxis_date()#this is to display actual dates in graph\n",
    "\n",
    "candlestick_ohlc(ax1,data_ohlc.values,width=5,colorup='g')\n",
    "ax2.fill_between(data_volume.index.map(mdates.date2num),data_volume.values,0)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using beautiful Soup to fetch S&P 500 companies from wikipedia.\n",
    "- import beautifulsoup4 library\n",
    "- use pickle to serialize any python object, this is used to save the S&P 500 list so as to avoid going back to wikipedia everytime it is needed.\n",
    "- fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp500():\n",
    "    res = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(res.text,'lxml')\n",
    "    table = soup.find('table',{'class':'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text #we need the first column which contains the tickers\n",
    "        tickers.append(ticker)\n",
    "    with open('sp500tickers.pickle',\"wb\")as f:\n",
    "        pickle.dump(tickers,f)#dumping tickers to file 'f'\n",
    "        print(tickers)\n",
    "    return tickers\n",
    "sp500()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Gettitng the data of all the companies we just fetched from wikipedia\n",
    "    - pull stock pricing data on all of the comapnies acquired\n",
    "    - if we chose to reload the data, the program will pull the stock information from wikipedia again. But we will be storing the data as a pickle object, and will be storing the data on our local system as the data will be fetched frequently, this makes it more efficient.\n",
    "    - Using the datetime function for pandas to specify dates for pandas.\n",
    "    - os is used to creaete, edit and delete files on the system.'\n",
    "    - Since I am unable to use the yahoo finance or google api to fetch historical data, I am using robinhood, it return the data from last year till today.\n",
    "\n",
    "In summary we are parsing the data once from the website and storing it locally. Time being another factor, as it takes quite some time to fetch the data again from the internet, storing the data locally provides faster data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''If reload is true then the fuction will reload the tickers from wikipedia\n",
    "else it will fetch the list of tickers from the pickle object '''\n",
    "\n",
    "def get_data(reload = False):\n",
    "    if reload:\n",
    "        tickers = sp500()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    #create a folder to save the csv files using OS\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "            os.mkdir('stock_dfs')\n",
    "    #fetching the historical stock price.    \n",
    "    start = dt.datetime(2000,1,1)\n",
    "    end = dt.datetime.now()\n",
    "    #iterating over the tickers to fectch the data for those in the list\n",
    "    for ticker in tickers:\n",
    "        print(ticker)\n",
    "        #is the csv file doesnt exist in the folder create one else acknowledge its existance\n",
    "        if not os.path.exists('stock_dfs\\{}.csv'.format(ticker)):\n",
    "            df = web.DataReader(ticker,'yahoo',start,end)\n",
    "            df.reset_index(inplace=True)\n",
    "            df.set_index(\"Date\",inplace=True)\n",
    "            df.to_csv('stock_dfs\\{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print(\"Alerady have {}\".format(ticker))\n",
    "get_data()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further we combine all the dataset into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compile_data():\n",
    "    with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "    \n",
    "    main_df = pd.DataFrame()\n",
    "    \n",
    "    for count,ticker in enumerate(tickers):\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "        \n",
    "#         df['{}_HL_pct_diff'.format(ticker)] = (df['High'] - df['Low']) / df['Low']\n",
    "#         df['{}_daily_pct_chng'.format(ticker)] = (df['Close'] - df['Open']) / df['Open']\n",
    "        \n",
    "        df.rename(columns = {'Adj Close':ticker},inplace=True)\n",
    "        df.drop(['Open','High','Low','Close','Volume'],1,inplace=True)\n",
    "        \n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df,how='outer')\n",
    "            \n",
    "        if count % 10==0:\n",
    "            print(count)\n",
    "    print(main_df.head(50))\n",
    "    main_df.to_csv('sp500_combo_adjc.csv')\n",
    "    \n",
    "compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is a correlation?<br>\n",
    "A correlation is a statistical test of association between variables that is measured on a -1 to 1 scale. The closer the correlation value is to -1 or 1 the stronger the association, the closer to 0, the weaker the association. It measures how change in one variable is associated with change in another variable.\n",
    "<br>\n",
    "There are a few common types of tests to measure the level of correlation, Pearson, Spearman, and Kendall. Each have their own assumptions about the data that needs to be meet in order for the test to be able to accurately measure the level of correlation. These are discussed further in the post. Each type of correlation test is testing the following hypothesis.\n",
    "\n",
    "Source: https://pythonfordatascience.org/correlation-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding correlations in the dataframe.\n",
    "- there are a lot of missing values  \n",
    "- attempting to look for some relationships in the dataset\n",
    "- data are more than 17 years old\n",
    "- the companies might have changed over the course of this period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-68-d3e2449400bb>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-68-d3e2449400bb>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    ax1.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def visuals():\n",
    "    df = pd.read_csv(\"sp500_combo_adjc.csv\")\n",
    "#     df['AAPL'].plot()\n",
    "#     plt.show()\n",
    "    df_corr = df.corr()\n",
    "    df_corr.to_csv('sp500corr.csv')\n",
    "    # fetching just the values\n",
    "    data1 = df.corr.values\n",
    "    #creating a figure\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.addsubplot(111)\n",
    "    #defining color for the heatmap\n",
    "    heatmap1 = ax1.pcolor(data1, cmap=plt.cm.RdYIGn)\n",
    "    fig1.colorbar(heatmap1)\n",
    "    \n",
    "    ax1.set_xticks(np.arrange(data1.shape[1]) + 0.5, minor=False)\n",
    "    ax1.set_yticks(np.arrange(data1.shape[0]) + 0.5, minor=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfect negative correlation** **(-1)** exists when the two securities move in opposite directions (i.e., stock A moves up and stock B moves down).<br>\n",
    "**Perfect positive correlation** **(+1)** exists if the two securities move in perfect unison (i.e., stock A and stock B move up and down at the same time).<br>\n",
    "**No correlation** **(0)** exists if the price movements are completely random (stock A and stock B go up and down randomly). (For more, see: What Does it Mean if the Correlation Coefficient is Positive, Negative or Zero?)\n",
    "\n",
    "Source:https://www.investopedia.com/university/guide-pairs-trading/pairs-trading-correlation.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
